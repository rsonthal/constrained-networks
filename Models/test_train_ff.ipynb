{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51338120-13b5-42f0-9fa2-6ff9bc52b153",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "import torch\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f9a8ab0-aea6-4955-9fcc-d75ea36ab88b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ train_ff.py contains 'flow_matching'\n",
      "\n",
      "============================================================\n",
      "Testing flow_matching on sphere\n",
      "============================================================\n",
      "\n",
      "/projects/gtml/Constrained Networks/src/Data/sphere_dataset.pt\n",
      "torch.Size([4000, 3])\n",
      "Loaded flow matching model from outputsflow/sphere_dataset/BEST/model.pt (input_dim=3)\n",
      "Epoch      1 | train_loss=1.923815e-01 | val_loss=1.905951e-01 | lr=1.000e-03\n",
      "\n",
      "Saved best checkpoint (epoch=1, best_val=1.905951e-01) to: outputs/sphere/flow_matching/depth2/out3/lr0.001_wd0.0001/seed0\n",
      "\n",
      "✓ sphere completed successfully\n",
      "\n",
      "============================================================\n",
      "Testing flow_matching on so3\n",
      "============================================================\n",
      "\n",
      "/projects/gtml/Constrained Networks/src/Data/so3_dataset.pt\n",
      "torch.Size([4000, 9])\n",
      "Loaded flow matching model from outputsflow/so3_dataset/BEST/model.pt (input_dim=9)\n",
      "Epoch      1 | train_loss=4.052885e-01 | val_loss=3.949543e-01 | lr=1.000e-03\n",
      "\n",
      "Saved best checkpoint (epoch=1, best_val=3.949543e-01) to: outputs/so3/flow_matching/depth2/out9/lr0.001_wd0.0001/seed0\n",
      "\n",
      "✓ so3 completed successfully\n",
      "\n",
      "============================================================\n",
      "Testing flow_matching on protein\n",
      "============================================================\n",
      "\n",
      "/projects/gtml/Constrained Networks/src/Data/protein_dataset.pt\n",
      "torch.Size([4000, 16])\n",
      "[protein] translation tau = 5152.61\n",
      "Loaded flow matching model from outputsflow/protein_dataset/BEST/model.pt (input_dim=16)\n",
      "Epoch      1 | train_loss=3.450849e-01 | val_loss=3.452888e-01 | lr=1.000e-03\n",
      "\n",
      "Saved best checkpoint (epoch=1, best_val=3.452888e-01) to: outputs/protein/flow_matching/depth2/out16/lr0.001_wd0.0001/seed0\n",
      "\n",
      "✓ protein completed successfully\n",
      "\n",
      "============================================================\n",
      "Testing flow_matching on disk\n",
      "============================================================\n",
      "\n",
      "/projects/gtml/Constrained Networks/src/Data/disk_dataset.pt\n",
      "torch.Size([4000, 2])\n",
      "Loaded flow matching model from outputsflow/disk_dataset/BEST/model.pt (input_dim=2)\n",
      "Epoch      1 | train_loss=3.656124e-01 | val_loss=3.662499e-01 | lr=1.000e-03\n",
      "\n",
      "Saved best checkpoint (epoch=1, best_val=3.662499e-01) to: outputs/disk/flow_matching/depth2/out2/lr0.001_wd0.0001/seed0\n",
      "\n",
      "✓ disk completed successfully\n"
     ]
    }
   ],
   "source": [
    "# Test flow matching on each dataset with minimal settings (1 epoch)\n",
    "# First, verify train_ff.py has flow_matching in choices\n",
    "import importlib.util\n",
    "spec = importlib.util.spec_from_file_location(\"train_ff_check\", \"train_ff.py\")\n",
    "if spec and spec.loader:\n",
    "    with open(\"train_ff.py\", \"r\") as f:\n",
    "        content = f.read()\n",
    "        if \"flow_matching\" in content:\n",
    "            print(\"✓ train_ff.py contains 'flow_matching'\")\n",
    "        else:\n",
    "            print(\"✗ train_ff.py does NOT contain 'flow_matching'\")\n",
    "            print(\"Please save the file and restart the kernel\")\n",
    "\n",
    "datasets = [\"sphere\", \"so3\", \"protein\", \"disk\"]\n",
    "\n",
    "for dataset in datasets:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Testing flow_matching on {dataset}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    cmd = [\n",
    "        sys.executable, \"train_ff.py\",\n",
    "        \"--model_type\", \"flow_matching\",\n",
    "        \"--dataset\", dataset,\n",
    "        \"--depth\", \"2\",\n",
    "        \"--num_epochs\", \"1\",\n",
    "        \"--batch_size\", \"100\",\n",
    "        \"--eval_every\", \"1\",\n",
    "        \"--lr\", \"1e-3\",\n",
    "        \"--device\", \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "        \"--residual\",\n",
    "        \"--use_internal\",\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        result = subprocess.run(cmd, capture_output=True, text=True, check=True, cwd=Path.cwd())\n",
    "        print(result.stdout)\n",
    "        if result.stderr:\n",
    "            print(\"STDERR:\", result.stderr)\n",
    "        print(f\"✓ {dataset} completed successfully\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"✗ {dataset} failed with return code {e.returncode}\")\n",
    "        print(\"STDOUT:\", e.stdout)\n",
    "        print(\"STDERR:\", e.stderr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7baf1a8-70c3-4a4f-ace4-81f1267ff825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ train_ff.py contains 'probabilistic'\n",
      "\n",
      "============================================================\n",
      "Testing probabilistic on sphere\n",
      "============================================================\n",
      "\n",
      "/projects/gtml/Constrained Networks/src/Data/sphere_dataset.pt\n",
      "torch.Size([4000, 3])\n",
      "--- Creating Anchors: Sampling 50 from Training Data ---\n",
      "Created 50 anchors with shape (50, 3)\n",
      "--- Generating Labels (Voronoi Partitioning) ---\n",
      "    Training Data (T): 4000\n",
      "    Anchors/Particles (N): 50\n",
      "    Done. Created DataLoader with 40 batches.\n",
      "epoch    1 | train 9.821e-01 | val 2.807e-01 | lr 1.0e-03\n",
      "\n",
      "Saved best checkpoint (epoch=0, best_val=2.807412e-01) to: outputs/sphere/probabilistic/depth2/out3/anchors50/lr0.001_wd0.0001/seed0\n",
      "\n",
      "✓ sphere completed successfully\n",
      "\n",
      "============================================================\n",
      "Testing probabilistic on so3\n",
      "============================================================\n",
      "\n",
      "/projects/gtml/Constrained Networks/src/Data/so3_dataset.pt\n",
      "torch.Size([4000, 9])\n",
      "--- Creating Anchors: Sampling 50 from Training Data ---\n",
      "Created 50 anchors with shape (50, 9)\n",
      "--- Generating Labels (Voronoi Partitioning) ---\n",
      "    Training Data (T): 4000\n",
      "    Anchors/Particles (N): 50\n",
      "    Done. Created DataLoader with 40 batches.\n",
      "epoch    1 | train 9.830e-01 | val 3.074e-01 | lr 1.0e-03\n",
      "\n",
      "Saved best checkpoint (epoch=0, best_val=3.073901e-01) to: outputs/so3/probabilistic/depth2/out9/anchors50/lr0.001_wd0.0001/seed0\n",
      "\n",
      "✓ so3 completed successfully\n",
      "\n",
      "============================================================\n",
      "Testing probabilistic on protein\n",
      "============================================================\n",
      "\n",
      "/projects/gtml/Constrained Networks/src/Data/protein_dataset.pt\n",
      "torch.Size([4000, 16])\n",
      "[protein] translation tau = 5152.61\n",
      "--- Creating Anchors: Sampling 50 from Training Data ---\n",
      "Created 50 anchors with shape (50, 16)\n",
      "--- Generating Labels (Voronoi Partitioning) ---\n",
      "    Training Data (T): 4000\n",
      "    Anchors/Particles (N): 50\n",
      "    Done. Created DataLoader with 40 batches.\n",
      "epoch    1 | train 9.822e-01 | val 3.367e-01 | lr 1.0e-03\n",
      "\n",
      "Saved best checkpoint (epoch=0, best_val=3.367097e-01) to: outputs/protein/probabilistic/depth2/out16/anchors50/lr0.001_wd0.0001/seed0\n",
      "\n",
      "✓ protein completed successfully\n",
      "\n",
      "============================================================\n",
      "Testing probabilistic on disk\n",
      "============================================================\n",
      "\n",
      "/projects/gtml/Constrained Networks/src/Data/disk_dataset.pt\n",
      "torch.Size([4000, 2])\n",
      "--- Creating Anchors: Sampling 50 from Training Data ---\n",
      "Created 50 anchors with shape (50, 2)\n",
      "--- Generating Labels (Voronoi Partitioning) ---\n",
      "    Training Data (T): 4000\n",
      "    Anchors/Particles (N): 50\n",
      "    Done. Created DataLoader with 40 batches.\n",
      "epoch    1 | train 9.853e-01 | val 4.052e-01 | lr 1.0e-03\n",
      "\n",
      "Saved best checkpoint (epoch=0, best_val=4.052466e-01) to: outputs/disk/probabilistic/depth2/out2/anchors50/lr0.001_wd0.0001/seed0\n",
      "\n",
      "✓ disk completed successfully\n"
     ]
    }
   ],
   "source": [
    "# Test probabilistic on each dataset with minimal settings (1 epoch)\n",
    "# First, verify train_ff.py has probabilistic in choices\n",
    "import importlib.util\n",
    "spec = importlib.util.spec_from_file_location(\"train_ff_check\", \"train_ff.py\")\n",
    "if spec and spec.loader:\n",
    "    with open(\"train_ff.py\", \"r\") as f:\n",
    "        content = f.read()\n",
    "        if \"probabilistic\" in content:\n",
    "            print(\"✓ train_ff.py contains 'probabilistic'\")\n",
    "        else:\n",
    "            print(\"✗ train_ff.py does NOT contain 'probabilistic'\")\n",
    "            print(\"Please save the file and restart the kernel\")\n",
    "\n",
    "datasets = [\"sphere\", \"so3\", \"protein\", \"disk\"]\n",
    "\n",
    "for dataset in datasets:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Testing probabilistic on {dataset}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    cmd = [\n",
    "        sys.executable, \"train_ff.py\",\n",
    "        \"--model_type\", \"probabilistic\",\n",
    "        \"--dataset\", dataset,\n",
    "        \"--depth\", \"2\",\n",
    "        \"--num_epochs\", \"1\",\n",
    "        \"--batch_size\", \"100\",\n",
    "        \"--eval_every\", \"1\",\n",
    "        \"--lr\", \"1e-3\",\n",
    "        \"--num_anchors\", \"50\",  # Use fewer anchors for quick test\n",
    "        \"--device\", \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "        \"--residual\",\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        result = subprocess.run(cmd, capture_output=True, text=True, check=True, cwd=Path.cwd())\n",
    "        print(result.stdout)\n",
    "        if result.stderr:\n",
    "            print(\"STDERR:\", result.stderr)\n",
    "        print(f\"✓ {dataset} completed successfully\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"✗ {dataset} failed with return code {e.returncode}\")\n",
    "        print(\"STDOUT:\", e.stdout)\n",
    "        print(\"STDERR:\", e.stderr)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
