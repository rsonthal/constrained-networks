{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d91e7f9f-ff52-41e7-a883-abb384e7b1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import json\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "# Import model building functions\n",
    "from train_ff import build_model, INPUT_DIMS, EXP_OUTPUT_DIMS\n",
    "from probabilistic import predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92d35c71-f0da-4956-912c-cdd462d7019c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "OUTPUTS_DIR = Path(\"outputs_final\") \n",
    "OUTPUTS_PROB = Path(\"outputs\") \n",
    "DATA_DIR = Path(\"../Data\") \n",
    "\n",
    "# Convert to absolute paths for clarity\n",
    "OUTPUTS_DIR = OUTPUTS_DIR.resolve()\n",
    "DATA_DIR = DATA_DIR.resolve()\n",
    "\n",
    "# Dataset configuration (from train_ff.py)\n",
    "DATASET_TO_FILE = {\n",
    "    \"sphere\": \"sphere_dataset.pt\",\n",
    "    \"disk\": \"disk_dataset.pt\",\n",
    "    \"so3\": \"so3_dataset.pt\",\n",
    "    \"cs\": \"cs_dataset.pt\",\n",
    "    \"protein\": \"protein_dataset.pt\",\n",
    "}\n",
    "\n",
    "INPUT_DIMS = {\n",
    "    \"sphere\": 3,\n",
    "    \"disk\": 2,\n",
    "    \"so3\": 9,\n",
    "    \"cs\": 9,\n",
    "    \"protein\": 16,\n",
    "}\n",
    "\n",
    "EXP_OUTPUT_DIMS = {\n",
    "    \"sphere\": 3,\n",
    "    \"so3\": 3,\n",
    "    \"cs\": 3,\n",
    "    \"protein\": 6,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fbf97a12-d5f8-4122-a027-920fb1756f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def _flatten_batch(X: torch.Tensor, d_last: int):\n",
    "    \"\"\"\n",
    "    X: [B,d] or [B,S,d]\n",
    "    Returns X_flat: [N,d], where N=B or B*S\n",
    "    \"\"\"\n",
    "    if X.dim() == 3:\n",
    "        B, S, D = X.shape\n",
    "        assert D == d_last, f\"Expected last dim {d_last}, got {D}\"\n",
    "        return X.reshape(B * S, D)\n",
    "    else:\n",
    "        B, D = X.shape\n",
    "        assert D == d_last, f\"Expected last dim {d_last}, got {D}\"\n",
    "        return X\n",
    "\n",
    "\n",
    "def sphere_distance_stats(X: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Distance to unit sphere: d(x,S^2) = | ||x|| - 1 |\n",
    "    X: [B,3] or [B,S,3]\n",
    "    Returns: (mean_dist, max_dist)\n",
    "    \"\"\"\n",
    "    X_flat = _flatten_batch(X, d_last=3)\n",
    "    dist = torch.abs(torch.linalg.norm(X_flat, dim=-1) - 1.0)\n",
    "    return dist.mean().item(), dist.max().item()\n",
    "\n",
    "\n",
    "def disk_distance_stats(X: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Distance to closed unit disk: d(x,D) = max(0, ||x|| - 1)\n",
    "    X: [B,2] or [B,S,2]\n",
    "    Returns: (mean_dist, max_dist)\n",
    "    \"\"\"\n",
    "    X_flat = _flatten_batch(X, d_last=2)\n",
    "    dist = torch.clamp(torch.linalg.norm(X_flat, dim=-1) - 1.0, min=0.0)\n",
    "    return dist.mean().item(), dist.max().item()\n",
    "\n",
    "\n",
    "def so3_distance_stats(R9: torch.Tensor):\n",
    "    \"\"\"\n",
    "    \"Distance-like\" constraint violations for SO(3), reporting two metrics:\n",
    "      - orth_dist = || R R^T - I ||_F\n",
    "      - det_dist  = |det(R) - 1|\n",
    "    R9: [B,9] or [B,S,9] (row-major flatten)\n",
    "    Returns:\n",
    "      (orth_mean, orth_max), (det_mean, det_max)\n",
    "    \"\"\"\n",
    "    R9_flat = _flatten_batch(R9, d_last=9)\n",
    "    R = R9_flat.reshape(-1, 3, 3)\n",
    "\n",
    "    I = torch.eye(3, device=R.device, dtype=R.dtype).expand(R.shape[0], 3, 3)\n",
    "    orth_dist = torch.linalg.norm(R @ R.transpose(-1, -2) - I, dim=(-2, -1))\n",
    "    det_dist = torch.abs(torch.linalg.det(R) - 1.0)\n",
    "\n",
    "    sum_dist = orth_dist + det_dist\n",
    "\n",
    "    return ((sum_dist.mean().item(), sum_dist.max().item()), \n",
    "            (orth_dist.mean().item(), orth_dist.max().item()), \n",
    "            (det_dist.mean().item(), det_dist.max().item()),)\n",
    "\n",
    "\n",
    "def se3_distance_stats(G16: torch.Tensor):\n",
    "    \"\"\"\n",
    "    \"Distance-like\" constraint violations for SE(3), reporting:\n",
    "      - orth_dist = || R R^T - I ||_F   (R is top-left 3x3)\n",
    "      - det_dist  = |det(R) - 1|\n",
    "      - last_row_dist = || last_row - [0,0,0,1] ||_inf\n",
    "    G16: [B,16] (row-major flatten)\n",
    "    Returns:\n",
    "      (orth_mean, orth_max), (det_mean, det_max), (last_mean, last_max)\n",
    "    \"\"\"\n",
    "    assert G16.dim() == 2 and G16.shape[1] == 16, \"Expected G16 shaped [B,16]\"\n",
    "    B = G16.shape[0]\n",
    "    G = G16.reshape(B, 4, 4)\n",
    "\n",
    "    R = G[:, :3, :3]\n",
    "    I = torch.eye(3, device=G.device, dtype=G.dtype).expand(B, 3, 3)\n",
    "    orth_dist = torch.linalg.norm(R @ R.transpose(-1, -2) - I, dim=(-2, -1))\n",
    "    det_dist = torch.abs(torch.linalg.det(R) - 1.0)\n",
    "\n",
    "    last = G[:, 3, :]\n",
    "    target = torch.tensor([0., 0., 0., 1.], device=G.device, dtype=G.dtype).expand_as(last)\n",
    "    last_dist = torch.max(torch.abs(last - target), dim=-1).values  # L_inf\n",
    "\n",
    "    sum_dist = orth_dist + det_dist + last_dist\n",
    "    return ((sum_dist.mean().item(), sum_dist.max().item()),\n",
    "            (orth_dist.mean().item(), orth_dist.max().item()),\n",
    "            (det_dist.mean().item(), det_dist.max().item()),\n",
    "            (last_dist.mean().item(), last_dist.max().item()))\n",
    "\n",
    "import torch\n",
    "\n",
    "def constraint_satisfaction(Y_pred: torch.Tensor, dataset: str):\n",
    "    \"\"\"\n",
    "    Returns constraint violation stats as plain Python floats so you can log them.\n",
    "\n",
    "    Args\n",
    "    ----\n",
    "    Y_pred:\n",
    "        Model outputs.\n",
    "        Expected shapes by dataset:\n",
    "          - \"sphere\":   [B,3] or [B,S,3]\n",
    "          - \"disk\":     [B,2] or [B,S,2]\n",
    "          - \"so3\":      [B,9] or [B,S,9]   (row-major 3x3)\n",
    "          - \"cs\":       [B,9] or [B,S,9]   (row-major 3x3)  (SO(3))\n",
    "          - \"protein\":  [B,16]             (row-major 4x4)  (SE(3))\n",
    "\n",
    "    dataset:\n",
    "        One of {\"sphere\",\"so3\",\"disk\",\"protein\",\"cs\"}.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    const: dict\n",
    "        A small dict of mean/max distances for the relevant constraints.\n",
    "        Example keys:\n",
    "          - sphere/disk: {\"mean_dist\": ..., \"max_dist\": ...}\n",
    "          - so3/cs:      {\"mean_orth_dist\": ..., \"max_orth_dist\": ..., \"mean_det_dist\": ..., \"max_det_dist\": ...}\n",
    "          - protein:     adds last-row stats as well.\n",
    "    \"\"\"\n",
    "    ds = dataset.lower().strip()\n",
    "\n",
    "    if ds == \"sphere\":\n",
    "        mean_d, max_d = sphere_distance_stats(Y_pred)\n",
    "        return {\"mean_dist\": mean_d, \"max_dist\": max_d}\n",
    "\n",
    "    if ds == \"disk\":\n",
    "        mean_d, max_d = disk_distance_stats(Y_pred)\n",
    "        return {\"mean_dist\": mean_d, \"max_dist\": max_d}\n",
    "\n",
    "    if ds in (\"so3\", \"cs\"):\n",
    "        (m_sum, M_sum), (m_orth, M_orth), (m_det, M_det) = so3_distance_stats(Y_pred)\n",
    "        return {\n",
    "            \"mean_sum_dist\": m_sum,\n",
    "            \"max_sum_dist\": M_sum,\n",
    "            \"mean_orth_dist\": m_orth,\n",
    "            \"max_orth_dist\": M_orth,\n",
    "            \"mean_det_dist\": m_det,\n",
    "            \"max_det_dist\": M_det,\n",
    "        }\n",
    "\n",
    "    if ds == \"protein\":\n",
    "        (m_sum, M_sum), (m_orth, M_orth), (m_det, M_det), (m_last, M_last) = se3_distance_stats(Y_pred)\n",
    "        return {\n",
    "            \"mean_sum_dist\": m_sum,\n",
    "            \"max_sum_dist\": M_sum,\n",
    "            \"mean_orth_dist\": m_orth,\n",
    "            \"max_orth_dist\": M_orth,\n",
    "            \"mean_det_dist\": m_det,\n",
    "            \"max_det_dist\": M_det,\n",
    "            \"mean_last_row_dist\": m_last,\n",
    "            \"max_last_row_dist\": M_last,\n",
    "        }\n",
    "\n",
    "    raise ValueError(f\"Unknown dataset='{dataset}'. Expected one of: sphere, so3, disk, protein, cs.\")\n",
    "\n",
    "def _format_constraints(const):\n",
    "    \"\"\"\n",
    "    const: dict returned by constraint_satisfaction(...)\n",
    "    Returns a pretty multi-line string.\n",
    "    \"\"\"\n",
    "    if const is None:\n",
    "        return \"  Constraints: (none)\"\n",
    "\n",
    "    # sphere/disk\n",
    "    if \"mean_dist\" in const and \"max_dist\" in const:\n",
    "        return (\n",
    "            \"  Constraints:\\n\"\n",
    "            f\"    dist-to-manifold  mean={const['mean_dist']:.3e}  max={const['max_dist']:.3e}\"\n",
    "        )\n",
    "\n",
    "    lines = [\"  Constraints:\"]\n",
    "    # SO(3) / CS\n",
    "    if \"mean_orth_dist\" in const:\n",
    "        lines.append(\n",
    "            f\"    ||RR^T-I||_F      mean={const['mean_orth_dist']:.3e}  max={const['max_orth_dist']:.3e}\"\n",
    "        )\n",
    "    if \"mean_det_dist\" in const:\n",
    "        lines.append(\n",
    "            f\"    |det(R)-1|        mean={const['mean_det_dist']:.3e}  max={const['max_det_dist']:.3e}\"\n",
    "        )\n",
    "    # SE(3) (protein)\n",
    "    if \"mean_last_row_dist\" in const:\n",
    "        lines.append(\n",
    "            f\"    last-row (L_inf)  mean={const['mean_last_row_dist']:.3e}  max={const['max_last_row_dist']:.3e}\"\n",
    "        )\n",
    "    if \"mean_sum_dist\" in const:\n",
    "        lines.append(\n",
    "            f\" SUM (all terms) mean={const['mean_sum_dist']:.3e} max={const['max_sum_dist']:.3e}\"\n",
    "        )\n",
    "\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "\n",
    "def _print_eval_summary(dataset, model_type, best_val_loss, best_dir, eval_results):\n",
    "    \"\"\"\n",
    "    eval_results is expected to contain:\n",
    "      - \"test_loss\": float\n",
    "      - \"constraints\": dict  (from constraint_satisfaction)\n",
    "    \"\"\"\n",
    "    test_loss = eval_results.get(\"test_loss\", None)\n",
    "    const = eval_results.get(\"constraints\", None)\n",
    "\n",
    "    print(f\"\\n{'-'*60}\")\n",
    "    print(f\"RESULT: {dataset} / {model_type}\")\n",
    "    print(f\"{'-'*60}\")\n",
    "    print(f\"  Best val loss: {best_val_loss:.6e}\")\n",
    "    print(f\"  Run dir:       {best_dir}\")\n",
    "    if test_loss is None:\n",
    "        print(\"  Test loss:     (missing)\")\n",
    "    else:\n",
    "        print(f\"  Test loss:     {test_loss:.6e}\")\n",
    "    print(_format_constraints(const))\n",
    "    print(f\"{'-'*60}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0d1168d-84b0-4bf6-8c89-bb9aec53f436",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_val_loss(run_dir: Path) -> Optional[float]:\n",
    "    \"\"\"\n",
    "    Extract best validation loss from meta.pt or meta.json.\n",
    "    \n",
    "    Returns the best validation loss (lowest = best performance).\n",
    "    \"\"\"\n",
    "    meta_pt = run_dir / \"meta.pt\"\n",
    "    meta_json = run_dir / \"meta.json\"\n",
    "    \n",
    "    # if meta_pt.exists():\n",
    "    #     try:\n",
    "    meta = torch.load(str(meta_pt), map_location=\"cpu\", weights_only=False)\n",
    "    logs = meta.get(\"logs\", {})\n",
    "    best_val = logs.get(\"best_val\")\n",
    "    if best_val is not None:\n",
    "        return float(best_val)\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "def find_best_run(dataset: str, model_type: str, outputs_dir: Path, verbose: bool = True) -> Optional[Tuple[Path, float]]:\n",
    "    \"\"\"\n",
    "    Find the run directory with the best validation loss for a given dataset/model_type.\n",
    "    \n",
    "    Handles various directory structures, including:\n",
    "    - outputs/{dataset}/{model_type}/depth{}/out{}/lr{}_wd{}/seed{}/\n",
    "    - outputs/{dataset}/{model_type}/internalTrue/depth{}/out{}/lr{}_wd{}/seed{}/\n",
    "    - outputs/{dataset}/probabilistic/depth{}/out{}/anchors{}/lr{}_wd{}/seed{}/\n",
    "    \n",
    "    Uses recursive search to find all model.pt files regardless of intermediate directory structure.\n",
    "    \"\"\"\n",
    "    dataset_dir = outputs_dir / dataset / model_type\n",
    "    \n",
    "    if not dataset_dir.exists():\n",
    "        if verbose:\n",
    "            print(f\"  Directory does not exist: {dataset_dir}\")\n",
    "        return None\n",
    "    \n",
    "    best_val = None\n",
    "    best_dir = None\n",
    "    all_runs = []  # Track all runs for debugging\n",
    "\n",
    "    # Only search depth* directories that are direct children of dataset_dir\n",
    "    depth_dirs = sorted([p for p in dataset_dir.iterdir() if p.is_dir() and p.name.startswith(\"depth\")])\n",
    "\n",
    "    \n",
    "    # Recursively search for all run directories (those containing model.pt)\n",
    "    for depth_dir in depth_dirs:\n",
    "        for model_pt_path in depth_dir.rglob(\"model.pt\"):\n",
    "            run_dir = model_pt_path.parent\n",
    "            val_loss = get_best_val_loss(run_dir)\n",
    "\n",
    "            if val_loss is not None:\n",
    "                all_runs.append((val_loss, run_dir))\n",
    "                if best_val is None or val_loss < best_val:\n",
    "                    best_val = val_loss\n",
    "                    best_dir = run_dir\n",
    "    \n",
    "    if best_dir is None:\n",
    "        return None\n",
    "    \n",
    "    return best_dir, best_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed64fbc8-d656-435d-9674-271fd9a8b04c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(PosixPath('/projects/gtml/Constrained Networks/src/Models/outputs_final/sphere/projected/internalFalse/depth8/out3/lr0.001_wd0/seed0'),\n",
       " 0.004208169380823771)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_best_run(\"sphere\", \"projected/internalFalse\", OUTPUTS_DIR) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f569add7-1d95-443c-b0ea-ab17ecd8db49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_and_meta(run_dir: Path, dataset: str, model_type: str, device: str = \"cpu\"):\n",
    "    \"\"\"Load model and metadata from a run directory\"\"\"\n",
    "    model_path = run_dir / \"model.pt\"\n",
    "    meta_path = run_dir / \"meta.pt\"\n",
    "    \n",
    "    if not model_path.exists():\n",
    "        raise FileNotFoundError(f\"Model not found: {model_path}\")\n",
    "    \n",
    "    # Load metadata\n",
    "    if meta_path.exists():\n",
    "        meta = torch.load(str(meta_path), map_location=\"cpu\", weights_only=False)\n",
    "    else:\n",
    "        meta = {}\n",
    "    \n",
    "    hparams = meta.get(\"hparams\", {})\n",
    "    \n",
    "    # Extract hyperparameters from saved metadata\n",
    "    depth = hparams.get(\"depth\", 2)\n",
    "    dropout = hparams.get(\"dropout\", 0.0)\n",
    "    residual = hparams.get(\"residual\", True)\n",
    "    dt = hparams.get(\"dt\", 1.0)\n",
    "    use_internal = hparams.get(\"use_internal\", False)\n",
    "    \n",
    "    # For CS dataset transformers, nhead and d_hid are needed but might not be in HParams\n",
    "    # Check if they're in the metadata directly (they might be saved separately)\n",
    "    nhead = meta.get(\"nhead\") or hparams.get(\"nhead\", 3)\n",
    "    d_hid = meta.get(\"d_hid\") or hparams.get(\"d_hid\", 2048)\n",
    "    \n",
    "    # Print hyperparameters being used (for verification)\n",
    "    # print(f\"  Building model with: depth={depth}, dropout={dropout}, residual={residual}, \"\n",
    "    #       f\"dt={dt}, use_internal={use_internal}, nhead={nhead}, d_hid={d_hid}\")\n",
    "    \n",
    "    # Determine output dimension\n",
    "    if model_type == \"exponential\":\n",
    "        d_out = EXP_OUTPUT_DIMS.get(dataset, INPUT_DIMS[dataset])\n",
    "    else:\n",
    "        d_out = INPUT_DIMS[dataset]\n",
    "    \n",
    "    # Build model\n",
    "    model = build_model(\n",
    "        model_type=model_type,\n",
    "        dataset=dataset,\n",
    "        depth=depth,\n",
    "        d_out=d_out,\n",
    "        dropout=dropout,\n",
    "        residual=residual,\n",
    "        dt=dt,\n",
    "        use_internal=use_internal,\n",
    "        outputsflow_dir=None,\n",
    "        nhead=nhead,\n",
    "        d_hid=d_hid,\n",
    "    )\n",
    "    \n",
    "    # Handle probabilistic model - need to set the final linear layer size\n",
    "    if model_type == \"probabilistic\":\n",
    "        num_anchors = meta.get(\"num_anchors\", 100)\n",
    "        if isinstance(model, nn.Sequential) and len(model) > 1:\n",
    "            # Replace the final linear layer\n",
    "            hidden_dim = model[0].output_dim if hasattr(model[0], 'output_dim') else d_out\n",
    "            model[-1] = nn.Linear(hidden_dim, num_anchors)\n",
    "    \n",
    "    # Load state dict\n",
    "    checkpoint = torch.load(str(model_path), map_location=device, weights_only=False)\n",
    "    state_dict = checkpoint.get(\"state_dict\", checkpoint)\n",
    "    model.load_state_dict(state_dict)\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    return model, meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b198821d-8e73-4ab5-8f9c-01fc412a2621",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_se3_translation(X16: torch.Tensor, tau: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    X16: [N,16] row-major flatten of 4x4.\n",
    "    Normalizes translation column (rows 0..2, col 3) by tau.\n",
    "    \"\"\"\n",
    "    G = X16.view(-1, 4, 4).clone()\n",
    "    G[:, :3, 3] = G[:, :3, 3] / tau\n",
    "    return G.view(-1, 16)\n",
    "\n",
    "\n",
    "def load_test_data(dataset: str, data_dir: Path, normalize_protein: bool = True) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Load test data for a dataset.\n",
    "    \n",
    "    For protein dataset, applies the same normalization as during training.\n",
    "    \"\"\"\n",
    "    data_file = data_dir / DATASET_TO_FILE[dataset]\n",
    "    \n",
    "    if not data_file.exists():\n",
    "        raise FileNotFoundError(f\"Data file not found: {data_file}\")\n",
    "    \n",
    "    loaded = torch.load(str(data_file), map_location=\"cpu\", weights_only=False)\n",
    "    \n",
    "    X_test = torch.tensor(loaded[\"X_test\"], dtype=torch.float32)\n",
    "    Y_test = torch.tensor(loaded[\"Y_test\"], dtype=torch.float32)\n",
    "    \n",
    "    # Apply protein normalization if needed (same as in train_ff.py)\n",
    "    if dataset == \"protein\" and normalize_protein:\n",
    "        # Compute tau from training data (same as during training)\n",
    "        X_train = torch.tensor(loaded[\"X_train\"], dtype=torch.float32)\n",
    "        t = X_train.view(-1, 4, 4)[:, :3, 3]  # Extract translation column\n",
    "        tau = t.std().clamp_min(1e-8)\n",
    "        \n",
    "        # Normalize test data with the same tau\n",
    "        X_test = normalize_se3_translation(X_test, tau)\n",
    "        Y_test = normalize_se3_translation(Y_test, tau)\n",
    "        \n",
    "        print(f\"  Applied protein normalization with tau={tau.item():.6g}\")\n",
    "    \n",
    "    return X_test, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "73b0b780-c79a-4f31-b0fe-b562fc538c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(\n",
    "    model: nn.Module,\n",
    "    X_test: torch.Tensor,\n",
    "    Y_test: torch.Tensor,\n",
    "    dataset: str,\n",
    "    model_type: str,\n",
    "    meta: Dict,\n",
    "    device: str = \"cpu\",\n",
    "    batch_size: int = 256\n",
    ") -> Dict:\n",
    "    \"\"\"Evaluate model on test data and compute loss + constraint satisfaction\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Handle probabilistic models specially\n",
    "    if model_type == \"probabilistic\":\n",
    "        print(meta.keys())\n",
    "        # Load anchors from metadata or recreate them\n",
    "        anchors_shape = meta.get(\"anchors_shape\")\n",
    "        num_anchors = meta.get(\"num_anchors\", 100)\n",
    "        anchors = torch.tensor(meta.get(\"anchors\")).to(device)\n",
    "    \n",
    "    # Move data to device\n",
    "    X_test = X_test.to(device)\n",
    "    Y_test = Y_test.to(device)\n",
    "    \n",
    "    # Compute predictions in batches\n",
    "    all_preds = []\n",
    "\n",
    "    if model_type == \"probabilistic\":\n",
    "        print(X_test.shape)\n",
    "        Y_pred = predict(model, X_test, anchors)\n",
    "        print(Y_pred.shape)\n",
    "    else:\n",
    "        Y_pred = model(X_test)\n",
    "    print(Y_test.shape)\n",
    "    loss = F.mse_loss(Y_pred, Y_test)\n",
    "    const = constraint_satisfaction(Y_pred, dataset)\n",
    "\n",
    "    return {\"test_loss\":loss, \"constraints\":const}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "98ea88d2-a808-4cf5-978d-cb100973a271",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "\n",
      "Searching for models in: /projects/gtml/Constrained Networks/src/Models/outputs_final\n",
      "Loading test data from: /projects/gtml/Constrained Networks/src/Data\n",
      "torch.Size([1200, 10, 9])\n",
      "\n",
      "------------------------------------------------------------\n",
      "RESULT: cs / regular\n",
      "------------------------------------------------------------\n",
      "  Best val loss: 3.323884e-01\n",
      "  Run dir:       /projects/gtml/Constrained Networks/src/Models/outputs_final/cs/regular/depth4/out9/lr0.001_wd0.0001/seed0\n",
      "  Test loss:     3.326826e-01\n",
      "  Constraints:\n",
      "    ||RR^T-I||_F      mean=1.722e+00  max=1.732e+00\n",
      "    |det(R)-1|        mean=9.998e-01  max=1.003e+00\n",
      " SUM (all terms) mean=2.721e+00 max=2.732e+00\n",
      "------------------------------------------------------------\n",
      "\n",
      "torch.Size([1200, 10, 9])\n",
      "\n",
      "------------------------------------------------------------\n",
      "RESULT: cs / exponential\n",
      "------------------------------------------------------------\n",
      "  Best val loss: 6.357073e-01\n",
      "  Run dir:       /projects/gtml/Constrained Networks/src/Models/outputs_final/cs/exponential/depth8/out3/lr0.001_wd0.0001/seed0\n",
      "  Test loss:     6.393778e-01\n",
      "  Constraints:\n",
      "    ||RR^T-I||_F      mean=2.923e-06  max=9.009e-06\n",
      "    |det(R)-1|        mean=1.855e-06  max=7.153e-06\n",
      " SUM (all terms) mean=4.779e-06 max=1.606e-05\n",
      "------------------------------------------------------------\n",
      "\n",
      "dict_keys(['dataset', 'train_shape', 'val_shape', 'test_shape', 'hparams', 'logs', 'num_anchors', 'anchors_shape', 'anchors'])\n",
      "torch.Size([1200, 10, 9])\n",
      "torch.Size([1200, 10, 9])\n",
      "torch.Size([1200, 10, 9])\n",
      "\n",
      "------------------------------------------------------------\n",
      "RESULT: cs / probabilistic\n",
      "------------------------------------------------------------\n",
      "  Best val loss: 3.320297e-01\n",
      "  Run dir:       outputs/cs/probabilistic/depth8/out9/anchors100/lr0.001_wd0/seed0\n",
      "  Test loss:     3.329417e-01\n",
      "  Constraints:\n",
      "    ||RR^T-I||_F      mean=1.724e+00  max=1.731e+00\n",
      "    |det(R)-1|        mean=9.997e-01  max=1.000e+00\n",
      " SUM (all terms) mean=2.724e+00 max=2.731e+00\n",
      "------------------------------------------------------------\n",
      "\n",
      "Loaded flow matching model from outputsflow/cs_dataset/BEST/model.pt (input_dim=9)\n",
      "torch.Size([1200, 10, 9])\n",
      "\n",
      "------------------------------------------------------------\n",
      "RESULT: cs / flow_matching\n",
      "------------------------------------------------------------\n",
      "  Best val loss: 3.351450e-01\n",
      "  Run dir:       /projects/gtml/Constrained Networks/src/Models/outputs_final/cs/flow_matching/depth4/out9/lr0.001_wd0/seed0\n",
      "  Test loss:     3.352439e-01\n",
      "  Constraints:\n",
      "    ||RR^T-I||_F      mean=1.714e+00  max=1.731e+00\n",
      "    |det(R)-1|        mean=9.997e-01  max=1.009e+00\n",
      " SUM (all terms) mean=2.713e+00 max=2.731e+00\n",
      "------------------------------------------------------------\n",
      "\n",
      "torch.Size([1200, 10, 9])\n",
      "\n",
      "------------------------------------------------------------\n",
      "RESULT: cs / projected/internalFalse\n",
      "------------------------------------------------------------\n",
      "  Best val loss: 6.310966e-01\n",
      "  Run dir:       /projects/gtml/Constrained Networks/src/Models/outputs_final/cs/projected/internalFalse/depth4/out9/lr0.001_wd0/seed0\n",
      "  Test loss:     6.383743e-01\n",
      "  Constraints:\n",
      "    ||RR^T-I||_F      mean=5.591e-07  max=1.870e-06\n",
      "    |det(R)-1|        mean=2.829e-07  max=1.550e-06\n",
      " SUM (all terms) mean=8.421e-07 max=3.329e-06\n",
      "------------------------------------------------------------\n",
      "\n",
      "torch.Size([1200, 10, 9])\n",
      "\n",
      "------------------------------------------------------------\n",
      "RESULT: cs / exponential/internalFalse\n",
      "------------------------------------------------------------\n",
      "  Best val loss: 6.331747e-01\n",
      "  Run dir:       /projects/gtml/Constrained Networks/src/Models/outputs_final/cs/exponential/internalFalse/depth4/out3/lr0.001_wd0.0001/seed0\n",
      "  Test loss:     6.361957e-01\n",
      "  Constraints:\n",
      "    ||RR^T-I||_F      mean=1.320e-06  max=4.704e-06\n",
      "    |det(R)-1|        mean=5.689e-07  max=3.219e-06\n",
      " SUM (all terms) mean=1.889e-06 max=7.379e-06\n",
      "------------------------------------------------------------\n",
      "\n",
      "Loaded flow matching model from outputsflow/cs_dataset/BEST/model.pt (input_dim=9)\n",
      "torch.Size([1200, 10, 9])\n",
      "\n",
      "------------------------------------------------------------\n",
      "RESULT: cs / flow_matching/internalFalse\n",
      "------------------------------------------------------------\n",
      "  Best val loss: 3.334058e-01\n",
      "  Run dir:       /projects/gtml/Constrained Networks/src/Models/outputs_final/cs/flow_matching/internalFalse/depth6/out9/lr0.001_wd0.0001/seed0\n",
      "  Test loss:     3.338823e-01\n",
      "  Constraints:\n",
      "    ||RR^T-I||_F      mean=1.720e+00  max=1.732e+00\n",
      "    |det(R)-1|        mean=9.999e-01  max=1.004e+00\n",
      " SUM (all terms) mean=2.720e+00 max=2.732e+00\n",
      "------------------------------------------------------------\n",
      "\n",
      "  Directory does not exist: /projects/gtml/Constrained Networks/src/Models/outputs_final/cs/flow_matching/internaTrue\n",
      "No runs found for cs/flow_matching/internaTrue\n"
     ]
    }
   ],
   "source": [
    "# Main extraction loop\n",
    "results = {}\n",
    "\n",
    "# Define all model types and datasets to check\n",
    "MODEL_TYPES = [\"regular\", \"exponential\", \"probabilistic\", \"flow_matching\", \"projected/internalFalse\", \"exponential/internalFalse\", \"flow_matching/internalFalse\", \"flow_matching/internaTrue\"]\n",
    "# MODEL_TYPES = [\"projected/depth8\"]\n",
    "\n",
    "DATASETS = [\"cs\"]\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "device = \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"\\nSearching for models in: {OUTPUTS_DIR}\")\n",
    "print(f\"Loading test data from: {DATA_DIR}\")\n",
    "\n",
    "for dataset in DATASETS:\n",
    "    results[dataset] = {}\n",
    "    \n",
    "    for model_type in MODEL_TYPES:\n",
    "        # print(f\"\\n{'='*60}\")\n",
    "        # print(f\"Processing: {dataset} / {model_type}\")\n",
    "        # print(f\"{'='*60}\")\n",
    "        \n",
    "        # Find best run\n",
    "        if model_type == \"probabilistic\":\n",
    "            best_result = find_best_run(dataset, model_type, OUTPUTS_PROB, verbose=True)\n",
    "        else:\n",
    "            best_result = find_best_run(dataset, model_type, OUTPUTS_DIR, verbose=True)\n",
    "        \n",
    "        if best_result is None:\n",
    "            print(f\"No runs found for {dataset}/{model_type}\")\n",
    "            results[dataset][model_type] = None\n",
    "            continue\n",
    "        \n",
    "        best_dir, best_val_loss = best_result\n",
    "        # print(f\"\\nSelected best run:\")\n",
    "        # print(f\"  Validation loss: {best_val_loss:.6e}\")\n",
    "        # print(f\"  Directory: {best_dir}\")\n",
    "        \n",
    "\n",
    "        # Load model and metadata\n",
    "        base = model_type.split(\"/\", 1)[0]\n",
    "        model, meta = load_model_and_meta(best_dir, dataset, base, device)\n",
    "        # print(f\"Model loaded successfully\")\n",
    "        \n",
    "        # Load test data (with normalization for protein dataset)\n",
    "        X_test, Y_test = load_test_data(dataset, DATA_DIR, normalize_protein=True)\n",
    "        # print(f\"Test data loaded: X_test.shape={X_test.shape}, Y_test.shape={Y_test.shape}\")\n",
    "        \n",
    "        # Evaluate model\n",
    "        eval_results = evaluate_model(model, X_test, Y_test, dataset, model_type, meta, device)\n",
    "        _print_eval_summary(dataset, model_type, best_val_loss, best_dir, eval_results)\n",
    "        results[dataset][model_type] = eval_results\n",
    "        # except:\n",
    "        #     print(\"error for\", dataset, model_type)\n",
    "        #     results[dataset][model_type] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb26831a-cc88-4334-9b31-f01bab230a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "latex = results_to_latex_table(results, DATASETS, MODEL_TYPES,\n",
    "                               caption=\"Test loss and constraint violation (mean / max).\",\n",
    "                               label=\"tab:outputs-final\")\n",
    "print(latex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174b99a8-f506-4e6e-9735-19fc0e6cf27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, Optional\n",
    "import math\n",
    "\n",
    "def _escape_latex(s: str) -> str:\n",
    "    # minimal escaping for your strings\n",
    "    return (\n",
    "        s.replace(\"\\\\\", \"\\\\textbackslash{}\")\n",
    "         .replace(\"_\", \"\\\\_\")\n",
    "         .replace(\"%\", \"\\\\%\")\n",
    "         .replace(\"&\", \"\\\\&\")\n",
    "         .replace(\"#\", \"\\\\#\")\n",
    "         .replace(\"{\", \"\\\\{\")\n",
    "         .replace(\"}\", \"\\\\}\")\n",
    "         .replace(\"^\", \"\\\\^{}\")\n",
    "         .replace(\"~\", \"\\\\~{}\")\n",
    "    )\n",
    "\n",
    "def _sci_latex(x: Optional[float], sig: int = 3) -> str:\n",
    "    \"\"\"Format float as LaTeX scientific notation like 1.23\\\\times10^{-4}.\"\"\"\n",
    "    if x is None or (isinstance(x, float) and (math.isnan(x) or math.isinf(x))):\n",
    "        return \"--\"\n",
    "    if x == 0:\n",
    "        return \"0\"\n",
    "    exp = int(math.floor(math.log10(abs(x))))\n",
    "    mant = x / (10 ** exp)\n",
    "    return f\"{mant:.{sig-1}f}\\\\times 10^{{{exp}}}\"\n",
    "\n",
    "def results_to_latex_table(\n",
    "    results: Dict[str, Dict[str, Any]],\n",
    "    datasets,\n",
    "    model_types,\n",
    "    caption: str = \"Test loss and constraint violation (mean / max).\",\n",
    "    label: str = \"tab:constraints\",\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Expects results[dataset][model_type] to be either:\n",
    "      - None (missing / not found / error), OR\n",
    "      - dict with keys: \"test_loss\" (float) and \"constraints\" (dict).\n",
    "    Produces one combined LaTeX table.\n",
    "    \"\"\"\n",
    "    lines = []\n",
    "    lines.append(\"\\\\begin{table}[t]\")\n",
    "    lines.append(\"\\\\centering\")\n",
    "    lines.append(\"\\\\small\")\n",
    "    lines.append(\"\\\\setlength{\\\\tabcolsep}{4pt}\")\n",
    "    lines.append(\"\\\\renewcommand{\\\\arraystretch}{1.15}\")\n",
    "    lines.append(\"\\\\begin{tabular}{llrcccccc}\")\n",
    "    lines.append(\"\\\\toprule\")\n",
    "    lines.append(\n",
    "        \"Dataset & Model & Test loss & \"\n",
    "        \"\\\\multicolumn{2}{c}{Dist} & \"\n",
    "        \"\\\\multicolumn{2}{c}{$\\\\|RR^\\\\top-I\\\\|_F$} & \"\n",
    "        \"\\\\multicolumn{2}{c}{$|\\\\det(R)-1|$} \\\\\\\\\"\n",
    "    )\n",
    "    lines.append(\n",
    "        \" & & & mean & max & mean & max & mean & max \\\\\\\\\"\n",
    "    )\n",
    "    lines.append(\"\\\\midrule\")\n",
    "\n",
    "    for ds in datasets:\n",
    "        first_row = True\n",
    "        for mt in model_types:\n",
    "            entry = results.get(ds, {}).get(mt, None)\n",
    "\n",
    "            if entry is None:\n",
    "                test = dist_mean = dist_max = orth_mean = orth_max = det_mean = det_max = \"--\"\n",
    "            else:\n",
    "                test_loss = entry.get(\"test_loss\", None)\n",
    "                const = entry.get(\"constraints\", None) or {}\n",
    "\n",
    "                test = _sci_latex(float(test_loss)) if test_loss is not None else \"--\"\n",
    "\n",
    "                # Defaults\n",
    "                dist_mean = dist_max = \"--\"\n",
    "                orth_mean = orth_max = \"--\"\n",
    "                det_mean = det_max = \"--\"\n",
    "\n",
    "                # sphere/disk\n",
    "                if \"mean_dist\" in const:\n",
    "                    dist_mean = _sci_latex(const.get(\"mean_dist\"))\n",
    "                    dist_max  = _sci_latex(const.get(\"max_dist\"))\n",
    "\n",
    "                # so3/cs/protein\n",
    "                if \"mean_orth_dist\" in const:\n",
    "                    orth_mean = _sci_latex(const.get(\"mean_orth_dist\"))\n",
    "                    orth_max  = _sci_latex(const.get(\"max_orth_dist\"))\n",
    "                if \"mean_det_dist\" in const:\n",
    "                    det_mean = _sci_latex(const.get(\"mean_det_dist\"))\n",
    "                    det_max  = _sci_latex(const.get(\"max_det_dist\"))\n",
    "\n",
    "            ds_cell = _escape_latex(ds) if first_row else \"\"\n",
    "            mt_cell = \"\\\\texttt{\" + _escape_latex(mt) + \"}\"\n",
    "            lines.append(\n",
    "                f\"{ds_cell} & {mt_cell} & {test} & \"\n",
    "                f\"{dist_mean} & {dist_max} & \"\n",
    "                f\"{orth_mean} & {orth_max} & \"\n",
    "                f\"{det_mean} & {det_max} \\\\\\\\\"\n",
    "            )\n",
    "            first_row = False\n",
    "\n",
    "        lines.append(\"\\\\midrule\")\n",
    "\n",
    "    # remove last midrule and replace with bottomrule\n",
    "    if lines[-1] == \"\\\\midrule\":\n",
    "        lines[-1] = \"\\\\bottomrule\"\n",
    "    else:\n",
    "        lines.append(\"\\\\bottomrule\")\n",
    "\n",
    "    lines.append(\"\\\\end{tabular}\")\n",
    "    lines.append(f\"\\\\caption{{{_escape_latex(caption)}}}\")\n",
    "    lines.append(f\"\\\\label{{{_escape_latex(label)}}}\")\n",
    "    lines.append(\"\\\\end{table}\")\n",
    "    return \"\\n\".join(lines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41bf38dd-ac23-42ae-a6ae-c529eeb93dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RESULTS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for dataset in DATASETS:\n",
    "    print(f\"\\n{dataset.upper()}:\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for model_type in MODEL_TYPES:\n",
    "        result = results.get(dataset, {}).get(model_type)\n",
    "        \n",
    "        if result is None:\n",
    "            print(f\"  {model_type:20s} - No runs found\")\n",
    "        elif \"error\" in result:\n",
    "            print(f\"  {model_type:20s} - ERROR: {result['error']}\")\n",
    "        else:\n",
    "            val_loss = result.get(\"best_val_loss\", \"N/A\")\n",
    "            test_loss = result.get(\"test_loss\", \"N/A\")\n",
    "            constraint = result.get(\"constraint_satisfaction\", {})\n",
    "            sat_rate = constraint.get(\"satisfaction_rate\", \"N/A\")\n",
    "            \n",
    "            print(f\"  {model_type:20s} - Val: {val_loss:.6e}, Test: {test_loss:.6e}, \"\n",
    "                  f\"Constraint: {sat_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3168ba3e-02b7-4378-b91a-2e59fe6a5d11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "FLOW MATCHING MODEL DEBUGGING\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "DEBUGGING FLOW MATCHING FOR DATASET: SO3\n",
      "================================================================================\n",
      "\n",
      "  Best run directory: /projects/gtml/Constrained Networks/src/Models/outputs/so3/flow_matching/depth8/out9/lr0.001_wd0/seed0\n",
      "  Best validation loss: 1.017315e-01\n",
      "\n",
      "  Loading model...\n",
      "  Building model with: depth=8, dropout=0.0, residual=True, dt=1.0, use_internal=True, nhead=3, d_hid=2048\n",
      "  Found outputsflow directory: /projects/gtml/Constrained Networks/src/Models/outputsflow\n",
      "  Flow matching model exists: /projects/gtml/Constrained Networks/src/Models/outputsflow/so3_dataset/BEST/model.pt\n",
      "Loaded flow matching model from /projects/gtml/Constrained Networks/src/Models/outputsflow/so3_dataset/BEST/model.pt (input_dim=9)\n",
      "\n",
      "  Checking projection function...\n",
      "    Model type: ProjectedFeedForward\n",
      "    Found final_proj_func (ProjectedFeedForward)\n",
      "  ✓ final_proj_func is set\n",
      "\n",
      "  Testing projection function directly...\n",
      "    Input shape: torch.Size([2, 9])\n",
      "    Output shape: torch.Size([2, 9])\n",
      "    L2 difference: 1.215075e+00\n",
      "    ✓ Projection changes values\n",
      "\n",
      "  Testing full forward pass...\n",
      "    Input shape: torch.Size([5, 9]) (should be [batch, features])\n",
      "    Prediction shape: torch.Size([5, 9])\n",
      "\n",
      "    Constraint check on first prediction:\n",
      "      Orthogonality error: 1.378963e+00\n",
      "      Determinant error: 9.702810e-01\n",
      "      ✗ Prediction is far from SO(3) - projection may not be working!\n",
      "\n",
      "    Computing constraint satisfaction on batch...\n",
      "      Samples satisfying constraints: 0/5\n",
      "      Max orthogonality error: 1.378963e+00\n",
      "      Max determinant error: 9.702810e-01\n",
      "\n",
      "  Testing projection on known manifold points...\n",
      "    Created valid SO(3) point, shape: torch.Size([1, 9])\n",
      "    Before projection: 1/1 satisfy constraints\n",
      "    After projection: 0/1 satisfy constraints\n",
      "    Max orthogonality error: 1.070691e+00\n",
      "    Max determinant error: 7.693483e-01\n",
      "    ✗ WARNING: Projection moved point OFF the manifold!\n",
      "    This suggests the flow matching projection is not working correctly.\n",
      "\n",
      "  Checking flow matching model source...\n",
      "    ⚠ outputsflow_dir not saved in metadata (we had to search for it)\n",
      "\n",
      "  Flow matching projection parameters:\n",
      "    T=2.0, num_steps=40, differentiable=True\n",
      "    ⚠ Using differentiable=True (Euler integration) may be less accurate than differentiable=False (scipy solver)\n",
      "\n",
      "================================================================================\n",
      "DEBUGGING FLOW MATCHING FOR DATASET: CS\n",
      "================================================================================\n",
      "\n",
      "  Best run directory: /projects/gtml/Constrained Networks/src/Models/outputs/cs/flow_matching/internalTrue/depth6/out9/lr0.001_wd0.0001/seed0\n",
      "  Best validation loss: 3.333832e-01\n",
      "\n",
      "  Loading model...\n",
      "  Building model with: depth=6, dropout=0.0, residual=True, dt=1.0, use_internal=True, nhead=3, d_hid=2048\n",
      "  Found outputsflow directory: /projects/gtml/Constrained Networks/src/Models/outputsflow\n",
      "  Flow matching model exists: /projects/gtml/Constrained Networks/src/Models/outputsflow/cs_dataset/BEST/model.pt\n",
      "Loaded flow matching model from /projects/gtml/Constrained Networks/src/Models/outputsflow/cs_dataset/BEST/model.pt (input_dim=9)\n",
      "\n",
      "  Checking projection function...\n",
      "    Model type: ProjectedTransformer\n",
      "    Found end_proj_func (ProjectedTransformer)\n",
      "  ✓ end_proj_func is set\n",
      "\n",
      "  Testing projection function directly...\n",
      "    Input shape: torch.Size([2, 10, 9])\n",
      "    Output shape: torch.Size([2, 10, 9])\n",
      "    L2 difference: 4.232270e+00\n",
      "    ✓ Projection changes values\n",
      "\n",
      "  Testing full forward pass...\n",
      "    Input shape: torch.Size([5, 10, 9]) (should be [batch, seq, features])\n",
      "    Prediction shape: torch.Size([5, 10, 9])\n",
      "\n",
      "    Constraint check on first prediction:\n",
      "      Orthogonality error: 1.730378e+00\n",
      "      Determinant error: 1.000016e+00\n",
      "      ✗ Prediction is far from SO(3) - projection may not be working!\n",
      "\n",
      "    Computing constraint satisfaction on batch...\n",
      "      Samples satisfying constraints: 0/5\n",
      "      Max orthogonality error: 1.731050e+00\n",
      "      Max determinant error: 1.000016e+00\n",
      "\n",
      "  Testing projection on known manifold points...\n",
      "    Created valid SO(3) point, shape: torch.Size([1, 10, 9])\n",
      "    Before projection: 10/1 satisfy constraints\n",
      "    After projection: 0/1 satisfy constraints\n",
      "    Max orthogonality error: 5.714119e-02\n",
      "    Max determinant error: 3.848028e-02\n",
      "    ✗ WARNING: Projection moved point OFF the manifold!\n",
      "    This suggests the flow matching projection is not working correctly.\n",
      "\n",
      "  Checking flow matching model source...\n",
      "    ⚠ outputsflow_dir not saved in metadata (we had to search for it)\n",
      "\n",
      "  Flow matching projection parameters:\n",
      "    T=2.0, num_steps=40, differentiable=True\n",
      "    ⚠ Using differentiable=True (Euler integration) may be less accurate than differentiable=False (scipy solver)\n",
      "\n",
      "================================================================================\n",
      "DEBUGGING FLOW MATCHING FOR DATASET: PROTEIN\n",
      "================================================================================\n",
      "\n",
      "  Best run directory: /projects/gtml/Constrained Networks/src/Models/outputs/protein/flow_matching/depth4/out16/lr0.001_wd0.0001/seed0\n",
      "  Best validation loss: 1.207986e-01\n",
      "\n",
      "  Loading model...\n",
      "  Building model with: depth=4, dropout=0.0, residual=True, dt=1.0, use_internal=True, nhead=3, d_hid=2048\n",
      "  Found outputsflow directory: /projects/gtml/Constrained Networks/src/Models/outputsflow\n",
      "  Flow matching model exists: /projects/gtml/Constrained Networks/src/Models/outputsflow/protein_dataset/BEST/model.pt\n",
      "Loaded flow matching model from /projects/gtml/Constrained Networks/src/Models/outputsflow/protein_dataset/BEST/model.pt (input_dim=16)\n",
      "\n",
      "  Checking projection function...\n",
      "    Model type: ProjectedFeedForward\n",
      "    Found final_proj_func (ProjectedFeedForward)\n",
      "  ✓ final_proj_func is set\n",
      "\n",
      "  Testing projection function directly...\n",
      "    Input shape: torch.Size([2, 16])\n",
      "    Output shape: torch.Size([2, 16])\n",
      "    L2 difference: 1.610358e-02\n",
      "    ✓ Projection changes values\n",
      "\n",
      "  Testing full forward pass...\n",
      "  Applied protein normalization with tau=5152.61\n",
      "    Input shape: torch.Size([5, 16]) (should be [batch, features])\n",
      "    Prediction shape: torch.Size([5, 16])\n",
      "\n",
      "    Computing constraint satisfaction on batch...\n",
      "      Using prediction shape: torch.Size([5, 16])\n",
      "      Samples satisfying constraints: 0/5\n",
      "      Max orthogonality error: 1.356014e+00\n",
      "      Max determinant error: 9.621895e-01\n",
      "\n",
      "  Testing projection on known manifold points...\n",
      "    Created valid SE(3) point, shape: torch.Size([1, 16])\n",
      "    Before projection: 1/1 satisfy constraints\n",
      "    After projection: 0/1 satisfy constraints\n",
      "    Max orthogonality error: 1.452854e-02\n",
      "    Max determinant error: 3.283143e-03\n",
      "    ✗ WARNING: Projection moved point OFF the manifold!\n",
      "    This suggests the flow matching projection is not working correctly.\n",
      "\n",
      "  Checking flow matching model source...\n",
      "    ⚠ outputsflow_dir not saved in metadata (we had to search for it)\n",
      "\n",
      "  Flow matching projection parameters:\n",
      "    T=2.0, num_steps=40, differentiable=True\n",
      "    ⚠ Using differentiable=True (Euler integration) may be less accurate than differentiable=False (scipy solver)\n",
      "\n",
      "================================================================================\n",
      "SUMMARY AND DIAGNOSIS\n",
      "================================================================================\n",
      "================================================================================\n",
      "DEBUGGING COMPLETE\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Debug Flow Matching Models\n",
    "# This cell tests flow matching models in detail to diagnose issues\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"FLOW MATCHING MODEL DEBUGGING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Test on a few datasets that had issues\n",
    "test_datasets = [\"so3\", \"cs\", \"protein\"]\n",
    "\n",
    "for dataset in test_datasets:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"DEBUGGING FLOW MATCHING FOR DATASET: {dataset.upper()}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    try:\n",
    "        # Find the best run\n",
    "        best_run = find_best_run(dataset, \"flow_matching\", OUTPUTS_DIR, verbose=False)\n",
    "        if best_run is None:\n",
    "            print(f\"  ✗ No flow matching model found for {dataset}\")\n",
    "            continue\n",
    "        \n",
    "        run_dir, val_loss = best_run\n",
    "        print(f\"  Best run directory: {run_dir}\")\n",
    "        print(f\"  Best validation loss: {val_loss:.6e}\\n\")\n",
    "        \n",
    "        # Load model and meta\n",
    "        print(\"  Loading model...\")\n",
    "        model, meta = load_model_and_meta(run_dir, dataset, \"flow_matching\", device=\"cpu\")\n",
    "        \n",
    "        # Check if projection function is set\n",
    "        # ProjectedTransformer uses 'end_proj_func', ProjectedFeedForward uses 'final_proj_func'\n",
    "        print(\"\\n  Checking projection function...\")\n",
    "        print(f\"    Model type: {type(model).__name__}\")\n",
    "        \n",
    "        proj_func = None\n",
    "        proj_attr_name = None\n",
    "        \n",
    "        if hasattr(model, 'end_proj_func'):\n",
    "            proj_func = model.end_proj_func\n",
    "            proj_attr_name = 'end_proj_func'\n",
    "            print(f\"    Found end_proj_func (ProjectedTransformer)\")\n",
    "        elif hasattr(model, 'final_proj_func'):\n",
    "            proj_func = model.final_proj_func\n",
    "            proj_attr_name = 'final_proj_func'\n",
    "            print(f\"    Found final_proj_func (ProjectedFeedForward)\")\n",
    "        else:\n",
    "            print(f\"    ✗ Model does not have end_proj_func or final_proj_func attribute!\")\n",
    "            print(f\"    Model attributes: {[attr for attr in dir(model) if 'proj' in attr.lower()]}\")\n",
    "        \n",
    "        if proj_func is not None:\n",
    "            print(f\"  ✓ {proj_attr_name} is set\")\n",
    "                \n",
    "                # Test projection function directly\n",
    "            print(\"\\n  Testing projection function directly...\")\n",
    "            if dataset == \"cs\":\n",
    "                    test_input = torch.randn(2, 10, INPUT_DIMS[dataset])\n",
    "            else:\n",
    "                    test_input = torch.randn(2, INPUT_DIMS[dataset])\n",
    "                \n",
    "            print(f\"    Input shape: {test_input.shape}\")\n",
    "                \n",
    "            with torch.no_grad():\n",
    "                    try:\n",
    "                        test_output = proj_func(test_input)\n",
    "                        print(f\"    Output shape: {test_output.shape}\")\n",
    "                        \n",
    "                        # Check if projection changes values\n",
    "                        diff = torch.norm(test_input - test_output).item()\n",
    "                        print(f\"    L2 difference: {diff:.6e}\")\n",
    "                        \n",
    "                        if diff < 1e-6:\n",
    "                            print(f\"    ⚠ WARNING: Projection appears to be identity!\")\n",
    "                        else:\n",
    "                            print(f\"    ✓ Projection changes values\")\n",
    "                        \n",
    "                        # Check for NaN/Inf\n",
    "                        if torch.isnan(test_output).any():\n",
    "                            print(f\"    ✗ Output contains NaN!\")\n",
    "                        if torch.isinf(test_output).any():\n",
    "                            print(f\"    ✗ Output contains Inf!\")\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        print(f\"    ✗ Projection function failed: {e}\")\n",
    "                        import traceback\n",
    "                        traceback.print_exc()\n",
    "        else:\n",
    "            print(f\"  ✗ {proj_attr_name} is None!\")\n",
    "        \n",
    "        # Test full forward pass\n",
    "        print(\"\\n  Testing full forward pass...\")\n",
    "        X_test, Y_test = load_test_data(dataset, DATA_DIR)\n",
    "        \n",
    "        # Take a small batch\n",
    "        batch_size = 5\n",
    "        X_batch = X_test[:batch_size]\n",
    "        Y_batch = Y_test[:batch_size]\n",
    "        \n",
    "        if dataset == \"cs\":\n",
    "            print(f\"    Input shape: {X_batch.shape} (should be [batch, seq, features])\")\n",
    "        else:\n",
    "            print(f\"    Input shape: {X_batch.shape} (should be [batch, features])\")\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            try:\n",
    "                pred = model(X_batch)\n",
    "                print(f\"    Prediction shape: {pred.shape}\")\n",
    "                \n",
    "                # Check for NaN/Inf\n",
    "                if torch.isnan(pred).any():\n",
    "                    print(f\"    ✗ Predictions contain NaN!\")\n",
    "                if torch.isinf(pred).any():\n",
    "                    print(f\"    ✗ Predictions contain Inf!\")\n",
    "                \n",
    "                # For SO3/CS, check constraint satisfaction on this small batch\n",
    "                if dataset in (\"so3\", \"cs\"):\n",
    "                    # Extract last timestep if sequential\n",
    "                    if pred.dim() == 3:\n",
    "                        pred_check = pred[:, -1, :]\n",
    "                    else:\n",
    "                        pred_check = pred\n",
    "                    \n",
    "                    # Reshape to 3x3 matrices\n",
    "                    if len(pred_check[0]) == 9:\n",
    "                        R = pred_check[0].reshape(3, 3)\n",
    "                        orth_err = torch.norm(R.T @ R - torch.eye(3)).item()\n",
    "                        det_err = abs(torch.det(R).item() - 1.0)\n",
    "                        print(f\"\\n    Constraint check on first prediction:\")\n",
    "                        print(f\"      Orthogonality error: {orth_err:.6e}\")\n",
    "                        print(f\"      Determinant error: {det_err:.6e}\")\n",
    "                        \n",
    "                        if orth_err > 0.1 or det_err > 0.1:\n",
    "                            print(f\"      ✗ Prediction is far from SO(3) - projection may not be working!\")\n",
    "                        else:\n",
    "                            print(f\"      ✓ Prediction is close to SO(3)\")\n",
    "                \n",
    "                # Check if projection was actually applied\n",
    "                # We can't easily intercept, but we can check if predictions satisfy constraints\n",
    "                print(f\"\\n    Computing constraint satisfaction on batch...\")\n",
    "                if dataset in (\"so3\", \"cs\"):\n",
    "                    ok, max_orth, max_det = check_so3_flat(pred_check)\n",
    "                    print(f\"      Samples satisfying constraints: {ok.sum().item()}/{len(ok)}\")\n",
    "                    print(f\"      Max orthogonality error: {max_orth:.6e}\")\n",
    "                    print(f\"      Max determinant error: {max_det:.6e}\")\n",
    "                elif dataset == \"protein\":\n",
    "                    # For protein, pred should be [batch, 16] (flattened 4x4 matrices)\n",
    "                    if pred.dim() == 2 and pred.shape[1] == 16:\n",
    "                        pred_check = pred\n",
    "                    elif pred.dim() == 3:\n",
    "                        # If sequential, take last timestep\n",
    "                        pred_check = pred[:, -1, :]\n",
    "                    else:\n",
    "                        print(f\"      ⚠ Unexpected prediction shape: {pred.shape}\")\n",
    "                        pred_check = pred\n",
    "                    \n",
    "                    print(f\"      Using prediction shape: {pred_check.shape}\")\n",
    "                    if pred_check.shape[1] != 16:\n",
    "                        print(f\"      ✗ ERROR: Expected 16 features for SE(3), got {pred_check.shape[1]}\")\n",
    "                    else:\n",
    "                        results = check_se3_flat(pred_check)\n",
    "                        print(f\"      Samples satisfying constraints: {results['ok'].sum().item()}/{len(results['ok'])}\")\n",
    "                        print(f\"      Max orthogonality error: {results['max_orth_err']:.6e}\")\n",
    "                        print(f\"      Max determinant error: {results['max_det_err']:.6e}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"    ✗ Forward pass failed: {e}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "        \n",
    "        # Test projection on known manifold points\n",
    "        print(\"\\n  Testing projection on known manifold points...\")\n",
    "        if dataset in (\"so3\", \"cs\"):\n",
    "            # Create a valid SO(3) rotation matrix\n",
    "            from scipy.spatial.transform import Rotation\n",
    "            R_valid = Rotation.random().as_matrix()\n",
    "            R9_valid = torch.tensor(R_valid.flatten(), dtype=torch.float32).unsqueeze(0)  # [1, 9]\n",
    "            \n",
    "            if dataset == \"cs\":\n",
    "                # For CS, need [batch, seq, features]\n",
    "                R9_valid = R9_valid.unsqueeze(1).expand(1, 10, 9)  # [1, 10, 9]\n",
    "            \n",
    "            print(f\"    Created valid SO(3) point, shape: {R9_valid.shape}\")\n",
    "            \n",
    "            # Check it satisfies constraints\n",
    "            ok_before, _, _ = check_so3_flat(R9_valid)\n",
    "            print(f\"    Before projection: {ok_before.sum().item()}/{len(ok_before)} satisfy constraints\")\n",
    "            \n",
    "            # Apply projection\n",
    "            with torch.no_grad():\n",
    "                R9_proj = proj_func(R9_valid)\n",
    "                ok_after, max_orth, max_det = check_so3_flat(R9_proj)\n",
    "                print(f\"    After projection: {ok_after.sum().item()}/{len(ok_after)} satisfy constraints\")\n",
    "                print(f\"    Max orthogonality error: {max_orth:.6e}\")\n",
    "                print(f\"    Max determinant error: {max_det:.6e}\")\n",
    "                \n",
    "                if ok_after.sum().item() < len(ok_after):\n",
    "                    print(f\"    ✗ WARNING: Projection moved point OFF the manifold!\")\n",
    "                    print(f\"    This suggests the flow matching projection is not working correctly.\")\n",
    "                else:\n",
    "                    print(f\"    ✓ Projection preserves manifold structure\")\n",
    "        \n",
    "        elif dataset == \"protein\":\n",
    "            # Create a valid SE(3) transformation\n",
    "            from scipy.spatial.transform import Rotation\n",
    "            R_valid = Rotation.random().as_matrix()\n",
    "            t_valid = np.random.randn(3)\n",
    "            G_valid = np.eye(4)\n",
    "            G_valid[:3, :3] = R_valid\n",
    "            G_valid[:3, 3] = t_valid\n",
    "            G16_valid = torch.tensor(G_valid.flatten(), dtype=torch.float32).unsqueeze(0)  # [1, 16]\n",
    "            \n",
    "            print(f\"    Created valid SE(3) point, shape: {G16_valid.shape}\")\n",
    "            \n",
    "            # Check it satisfies constraints\n",
    "            results_before = check_se3_flat(G16_valid)\n",
    "            print(f\"    Before projection: {results_before['ok'].sum().item()}/{len(results_before['ok'])} satisfy constraints\")\n",
    "            \n",
    "            # Apply projection\n",
    "            with torch.no_grad():\n",
    "                G16_proj = proj_func(G16_valid)\n",
    "                results_after = check_se3_flat(G16_proj)\n",
    "                print(f\"    After projection: {results_after['ok'].sum().item()}/{len(results_after['ok'])} satisfy constraints\")\n",
    "                print(f\"    Max orthogonality error: {results_after['max_orth_err']:.6e}\")\n",
    "                print(f\"    Max determinant error: {results_after['max_det_err']:.6e}\")\n",
    "                \n",
    "                if results_after['ok'].sum().item() < len(results_after['ok']):\n",
    "                    print(f\"    ✗ WARNING: Projection moved point OFF the manifold!\")\n",
    "                    print(f\"    This suggests the flow matching projection is not working correctly.\")\n",
    "                else:\n",
    "                    print(f\"    ✓ Projection preserves manifold structure\")\n",
    "        \n",
    "        # Check what flow matching model was loaded\n",
    "        print(\"\\n  Checking flow matching model source...\")\n",
    "        # The flow_model is captured in the closure, so we can't easily inspect it\n",
    "        # But we can check if outputsflow_dir was used correctly\n",
    "        hparams = meta.get(\"hparams\", {})\n",
    "        saved_outputsflow = hparams.get(\"outputsflow_dir\") or meta.get(\"outputsflow_dir\")\n",
    "        if saved_outputsflow:\n",
    "            print(f\"    Saved outputsflow_dir: {saved_outputsflow}\")\n",
    "        else:\n",
    "            print(f\"    ⚠ outputsflow_dir not saved in metadata (we had to search for it)\")\n",
    "        \n",
    "        # Check projection parameters\n",
    "        print(\"\\n  Flow matching projection parameters:\")\n",
    "        print(f\"    T=2.0, num_steps=40, differentiable=True\")\n",
    "        print(f\"    ⚠ Using differentiable=True (Euler integration) may be less accurate than differentiable=False (scipy solver)\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ✗ Error debugging {dataset}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SUMMARY AND DIAGNOSIS\")\n",
    "print(\"=\" * 80)\n",
    "print(\"=\" * 80)\n",
    "print(\"DEBUGGING COMPLETE\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b3ad4d95-dc02-4017-b66d-0acef3f0fde9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CHECKING CONSTRAINT SATISFACTION ON TRAINING DATA\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "DATASET: SPHERE\n",
      "================================================================================\n",
      "\n",
      "  Loading training data...\n",
      "  Training data shape: X=torch.Size([4000, 3]), Y=torch.Size([4000, 3])\n",
      "\n",
      "  Checking constraint satisfaction on ground truth Y_train...\n",
      "\n",
      "  Loading best model and checking predictions on training data...\n",
      "  Building model with: depth=6, dropout=0.0, residual=True, dt=1.0, use_internal=True, nhead=3, d_hid=2048\n",
      "  Building model with: depth=8, dropout=0.0, residual=True, dt=1.0, use_internal=False, nhead=3, d_hid=2048\n",
      "  Building model with: depth=8, dropout=0.0, residual=True, dt=1.0, use_internal=True, nhead=3, d_hid=2048\n",
      "<function sphere at 0x15544641f250> 3 3 8 0.0 True\n",
      "    probabilistic: Skipping (requires anchors)\n",
      "  Building model with: depth=8, dropout=0.0, residual=True, dt=1.0, use_internal=False, nhead=3, d_hid=2048\n",
      "  Found outputsflow directory: /projects/gtml/Constrained Networks/src/Models/outputsflow\n",
      "  Flow matching model exists: /projects/gtml/Constrained Networks/src/Models/outputsflow/sphere_dataset/BEST/model.pt\n",
      "Loaded flow matching model from /projects/gtml/Constrained Networks/src/Models/outputsflow/sphere_dataset/BEST/model.pt (input_dim=3)\n",
      "\n",
      "================================================================================\n",
      "DATASET: DISK\n",
      "================================================================================\n",
      "\n",
      "  Loading training data...\n",
      "  Training data shape: X=torch.Size([4000, 2]), Y=torch.Size([4000, 2])\n",
      "\n",
      "  Checking constraint satisfaction on ground truth Y_train...\n",
      "\n",
      "  Loading best model and checking predictions on training data...\n",
      "  Building model with: depth=8, dropout=0.0, residual=True, dt=1.0, use_internal=True, nhead=3, d_hid=2048\n",
      "  Building model with: depth=8, dropout=0.0, residual=True, dt=1.0, use_internal=True, nhead=3, d_hid=2048\n",
      "    exponential: No model found\n",
      "    probabilistic: Skipping (requires anchors)\n",
      "  Building model with: depth=8, dropout=0.0, residual=True, dt=1.0, use_internal=False, nhead=3, d_hid=2048\n",
      "  Found outputsflow directory: /projects/gtml/Constrained Networks/src/Models/outputsflow\n",
      "  Flow matching model exists: /projects/gtml/Constrained Networks/src/Models/outputsflow/disk_dataset/BEST/model.pt\n",
      "Loaded flow matching model from /projects/gtml/Constrained Networks/src/Models/outputsflow/disk_dataset/BEST/model.pt (input_dim=2)\n",
      "\n",
      "================================================================================\n",
      "DATASET: SO3\n",
      "================================================================================\n",
      "\n",
      "  Loading training data...\n",
      "  Training data shape: X=torch.Size([4000, 9]), Y=torch.Size([4000, 9])\n",
      "\n",
      "  Checking constraint satisfaction on ground truth Y_train...\n",
      "    Samples satisfying constraints: 4000/4000 (100.00%)\n",
      "    Max orthogonality error: 2.107342e-07\n",
      "    Max determinant error: 2.384186e-07\n",
      "    ✓ Training data satisfies SO(3) constraints\n",
      "\n",
      "  Loading best model and checking predictions on training data...\n",
      "  Building model with: depth=8, dropout=0.0, residual=True, dt=1.0, use_internal=True, nhead=3, d_hid=2048\n",
      "    regular             : 0/100 (0.00%) satisfy constraints\n",
      "      Max errors: orth=1.410557e+00, det=1.054899e+00\n",
      "  Building model with: depth=6, dropout=0.0, residual=True, dt=1.0, use_internal=True, nhead=3, d_hid=2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1171115/3370561147.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_train = torch.tensor(loaded[\"X_train\"], dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    projected           : 100/100 (100.00%) satisfy constraints\n",
      "      Max errors: orth=1.256743e-06, det=1.072884e-06\n",
      "  Building model with: depth=4, dropout=0.0, residual=True, dt=1.0, use_internal=False, nhead=3, d_hid=2048\n",
      "<function so3 at 0x15544641d6c0> 9 3 4 0.0 False\n",
      "    exponential         : 100/100 (100.00%) satisfy constraints\n",
      "      Max errors: orth=4.466861e-06, det=3.218651e-06\n",
      "    probabilistic: Skipping (requires anchors)\n",
      "  Building model with: depth=8, dropout=0.0, residual=True, dt=1.0, use_internal=True, nhead=3, d_hid=2048\n",
      "  Found outputsflow directory: /projects/gtml/Constrained Networks/src/Models/outputsflow\n",
      "  Flow matching model exists: /projects/gtml/Constrained Networks/src/Models/outputsflow/so3_dataset/BEST/model.pt\n",
      "Loaded flow matching model from /projects/gtml/Constrained Networks/src/Models/outputsflow/so3_dataset/BEST/model.pt (input_dim=9)\n",
      "    flow_matching       : 0/100 (0.00%) satisfy constraints\n",
      "      Max errors: orth=1.413097e+00, det=1.001283e+00\n",
      "\n",
      "================================================================================\n",
      "DATASET: CS\n",
      "================================================================================\n",
      "\n",
      "  Loading training data...\n",
      "  Training data shape: X=torch.Size([4000, 10, 9]), Y=torch.Size([4000, 10, 9])\n",
      "\n",
      "  Checking constraint satisfaction on ground truth Y_train...\n",
      "    Samples satisfying constraints: 4000/4000 (100.00%)\n",
      "    Max orthogonality error: 1.890745e-07\n",
      "    Max determinant error: 2.384186e-07\n",
      "    ✓ Training data satisfies SO(3) constraints\n",
      "\n",
      "  Loading best model and checking predictions on training data...\n",
      "  Building model with: depth=4, dropout=0.0, residual=True, dt=1.0, use_internal=True, nhead=3, d_hid=2048\n",
      "    regular             : 0/100 (0.00%) satisfy constraints\n",
      "      Max errors: orth=1.731126e+00, det=1.000618e+00\n",
      "  Building model with: depth=4, dropout=0.0, residual=True, dt=1.0, use_internal=False, nhead=3, d_hid=2048\n",
      "    projected           : 100/100 (100.00%) satisfy constraints\n",
      "      Max errors: orth=1.207638e-06, det=7.748604e-07\n",
      "  Building model with: depth=4, dropout=0.0, residual=True, dt=1.0, use_internal=False, nhead=3, d_hid=2048\n",
      "    exponential         : 100/100 (100.00%) satisfy constraints\n",
      "      Max errors: orth=3.064155e-06, det=2.503395e-06\n",
      "    probabilistic: Skipping (requires anchors)\n",
      "  Building model with: depth=6, dropout=0.0, residual=True, dt=1.0, use_internal=True, nhead=3, d_hid=2048\n",
      "  Found outputsflow directory: /projects/gtml/Constrained Networks/src/Models/outputsflow\n",
      "  Flow matching model exists: /projects/gtml/Constrained Networks/src/Models/outputsflow/cs_dataset/BEST/model.pt\n",
      "Loaded flow matching model from /projects/gtml/Constrained Networks/src/Models/outputsflow/cs_dataset/BEST/model.pt (input_dim=9)\n",
      "    flow_matching       : 0/100 (0.00%) satisfy constraints\n",
      "      Max errors: orth=1.731065e+00, det=1.002167e+00\n",
      "\n",
      "================================================================================\n",
      "DATASET: PROTEIN\n",
      "================================================================================\n",
      "\n",
      "  Loading training data...\n",
      "  Applied protein normalization with tau=5152.61\n",
      "  Training data shape: X=torch.Size([4000, 16]), Y=torch.Size([4000, 16])\n",
      "\n",
      "  Checking constraint satisfaction on ground truth Y_train...\n",
      "    Samples satisfying constraints: 3978/4000 (99.45%)\n",
      "    Max orthogonality error: 1.414115e+00\n",
      "    Max determinant error: 9.881887e-01\n",
      "    ✓ Training data satisfies SE(3) constraints\n",
      "\n",
      "  Loading best model and checking predictions on training data...\n",
      "  Building model with: depth=4, dropout=0.0, residual=True, dt=1.0, use_internal=True, nhead=3, d_hid=2048\n",
      "    regular             : 0/100 (0.00%) satisfy constraints\n",
      "      Max errors: orth=1.393566e+00, det=9.912599e-01\n",
      "  Building model with: depth=4, dropout=0.0, residual=True, dt=1.0, use_internal=True, nhead=3, d_hid=2048\n",
      "    projected           : 100/100 (100.00%) satisfy constraints\n",
      "      Max errors: orth=1.244760e-06, det=9.536743e-07\n",
      "  Building model with: depth=4, dropout=0.0, residual=True, dt=1.0, use_internal=True, nhead=3, d_hid=2048\n",
      "<function se3 at 0x15544641d630> 16 6 4 0.0 True\n",
      "    exponential         : 99/100 (99.00%) satisfy constraints\n",
      "      Max errors: orth=1.333357e+00, det=6.667125e-01\n",
      "    probabilistic: Skipping (requires anchors)\n",
      "  Building model with: depth=4, dropout=0.0, residual=True, dt=1.0, use_internal=True, nhead=3, d_hid=2048\n",
      "  Found outputsflow directory: /projects/gtml/Constrained Networks/src/Models/outputsflow\n",
      "  Flow matching model exists: /projects/gtml/Constrained Networks/src/Models/outputsflow/protein_dataset/BEST/model.pt\n",
      "Loaded flow matching model from /projects/gtml/Constrained Networks/src/Models/outputsflow/protein_dataset/BEST/model.pt (input_dim=16)\n",
      "    flow_matching       : 0/100 (0.00%) satisfy constraints\n",
      "      Max errors: orth=1.379840e+00, det=9.785105e-01\n",
      "\n",
      "================================================================================\n",
      "TRAINING DATA CONSTRAINT CHECK COMPLETE\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Check constraint satisfaction on training data\n",
    "print(\"=\" * 80)\n",
    "print(\"CHECKING CONSTRAINT SATISFACTION ON TRAINING DATA\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Define load_train_data if not already defined (should be in cell 6)\n",
    "if 'load_train_data' not in globals():\n",
    "    def load_train_data(dataset: str, data_dir: Path, normalize_protein: bool = True) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Load training data for a dataset.\n",
    "        \n",
    "        For protein dataset, applies the same normalization as during training.\n",
    "        \"\"\"\n",
    "        data_file = data_dir / DATASET_TO_FILE[dataset]\n",
    "        \n",
    "        if not data_file.exists():\n",
    "            raise FileNotFoundError(f\"Data file not found: {data_file}\")\n",
    "        \n",
    "        loaded = torch.load(str(data_file), map_location=\"cpu\", weights_only=False)\n",
    "        \n",
    "        X_train = torch.tensor(loaded[\"X_train\"], dtype=torch.float32)\n",
    "        Y_train = torch.tensor(loaded[\"Y_train\"], dtype=torch.float32)\n",
    "        \n",
    "        # Apply protein normalization if needed (same as in train_ff.py)\n",
    "        if dataset == \"protein\" and normalize_protein:\n",
    "            # Compute tau from training data (same as during training)\n",
    "            t = X_train.view(-1, 4, 4)[:, :3, 3]  # Extract translation column\n",
    "            tau = t.std().clamp_min(1e-8)\n",
    "            \n",
    "            # Normalize training data with the same tau\n",
    "            X_train = normalize_se3_translation(X_train, tau)\n",
    "            Y_train = normalize_se3_translation(Y_train, tau)\n",
    "            \n",
    "            print(f\"  Applied protein normalization with tau={tau.item():.6g}\")\n",
    "        \n",
    "        return X_train, Y_train\n",
    "\n",
    "# Test on all datasets\n",
    "for dataset in DATASETS:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"DATASET: {dataset.upper()}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    try:\n",
    "        # Load training data\n",
    "        print(\"  Loading training data...\")\n",
    "        X_train, Y_train = load_train_data(dataset, DATA_DIR)\n",
    "        print(f\"  Training data shape: X={X_train.shape}, Y={Y_train.shape}\")\n",
    "        \n",
    "        # Check constraint satisfaction on ground truth Y_train\n",
    "        print(\"\\n  Checking constraint satisfaction on ground truth Y_train...\")\n",
    "        if dataset in (\"so3\", \"cs\"):\n",
    "            # For CS, check last timestep\n",
    "            if Y_train.dim() == 3:\n",
    "                Y_check = Y_train[:, -1, :]\n",
    "            else:\n",
    "                Y_check = Y_train\n",
    "            \n",
    "            ok, max_orth, max_det = check_so3_flat(Y_check)\n",
    "            sat_rate = ok.float().mean().item()\n",
    "            print(f\"    Samples satisfying constraints: {ok.sum().item()}/{len(ok)} ({sat_rate*100:.2f}%)\")\n",
    "            print(f\"    Max orthogonality error: {max_orth:.6e}\")\n",
    "            print(f\"    Max determinant error: {max_det:.6e}\")\n",
    "            \n",
    "            if sat_rate < 0.99:\n",
    "                print(f\"    ⚠ WARNING: Training data does not fully satisfy SO(3) constraints!\")\n",
    "            else:\n",
    "                print(f\"    ✓ Training data satisfies SO(3) constraints\")\n",
    "                \n",
    "        elif dataset == \"protein\":\n",
    "            results = check_se3_flat(Y_train)\n",
    "            sat_rate = results[\"ok\"].float().mean().item()\n",
    "            print(f\"    Samples satisfying constraints: {results['ok'].sum().item()}/{len(results['ok'])} ({sat_rate*100:.2f}%)\")\n",
    "            print(f\"    Max orthogonality error: {results['max_orth_err']:.6e}\")\n",
    "            print(f\"    Max determinant error: {results['max_det_err']:.6e}\")\n",
    "            \n",
    "            if sat_rate < 0.99:\n",
    "                print(f\"    ⚠ WARNING: Training data does not fully satisfy SE(3) constraints!\")\n",
    "            else:\n",
    "                print(f\"    ✓ Training data satisfies SE(3) constraints\")\n",
    "        \n",
    "        # Load best model and check predictions on training data\n",
    "        print(\"\\n  Loading best model and checking predictions on training data...\")\n",
    "        for model_type in MODEL_TYPES:\n",
    "            if model_type == \"probabilistic\":\n",
    "                print(f\"    {model_type}: Skipping (requires anchors)\")\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                best_run = find_best_run(dataset, model_type, OUTPUTS_DIR, verbose=False)\n",
    "                if best_run is None:\n",
    "                    print(f\"    {model_type}: No model found\")\n",
    "                    continue\n",
    "                \n",
    "                run_dir, val_loss = best_run\n",
    "                model, meta = load_model_and_meta(run_dir, dataset, model_type, device=\"cpu\")\n",
    "                \n",
    "                # Evaluate on a sample of training data (to avoid memory issues)\n",
    "                sample_size = min(100, len(X_train))\n",
    "                indices = torch.randperm(len(X_train))[:sample_size]\n",
    "                X_sample = X_train[indices]\n",
    "                Y_sample = Y_train[indices]\n",
    "                \n",
    "                # Get predictions\n",
    "                with torch.no_grad():\n",
    "                    pred = model(X_sample)\n",
    "                    \n",
    "                    # For sequential data, extract last timestep\n",
    "                    if dataset == \"cs\" and pred.dim() == 3:\n",
    "                        pred_check = pred[:, -1, :]\n",
    "                        Y_check = Y_sample[:, -1, :] if Y_sample.dim() == 3 else Y_sample\n",
    "                    else:\n",
    "                        pred_check = pred\n",
    "                        Y_check = Y_sample\n",
    "                \n",
    "                # Check constraint satisfaction on predictions\n",
    "                if dataset in (\"so3\", \"cs\"):\n",
    "                    ok, max_orth, max_det = check_so3_flat(pred_check)\n",
    "                    sat_rate = ok.float().mean().item()\n",
    "                    print(f\"    {model_type:20s}: {ok.sum().item()}/{len(ok)} ({sat_rate*100:.2f}%) satisfy constraints\")\n",
    "                    print(f\"      Max errors: orth={max_orth:.6e}, det={max_det:.6e}\")\n",
    "                elif dataset == \"protein\":\n",
    "                    results = check_se3_flat(pred_check)\n",
    "                    sat_rate = results[\"ok\"].float().mean().item()\n",
    "                    print(f\"    {model_type:20s}: {results['ok'].sum().item()}/{len(results['ok'])} ({sat_rate*100:.2f}%) satisfy constraints\")\n",
    "                    print(f\"      Max errors: orth={results['max_orth_err']:.6e}, det={results['max_det_err']:.6e}\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"    {model_type}: Error - {e}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ✗ Error processing {dataset}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TRAINING DATA CONSTRAINT CHECK COMPLETE\")\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
